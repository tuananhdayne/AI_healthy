import os
import re
import pickle
import faiss
from sentence_transformers import SentenceTransformer

# ================================
# 1) PATH
# ================================

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(BASE_DIR, "data")
EMB_DIR = os.path.join(BASE_DIR, "embeddings")

os.makedirs(EMB_DIR, exist_ok=True)

# Intent â†’ filename mapping
INTENT_FILES = {
    "bao_dau_bung": "bao_dau_bung.txt",
    "bao_dau_dau": "bao_dau_dau.txt",
    "bao_ho": "bao_ho.txt",
    "bao_met_moi": "bao_met_moi.txt",
    "bao_sot": "bao_sot.txt",
    "lo_lang_stress": "lo_lang_stress.txt",
    "tu_van_dinh_duong": "tu_van_dinh_duong.txt",
    "tu_van_tap_luyen": "tu_van_tap_luyen.txt",
    # cÃ³ thá»ƒ thÃªm intent má»›i sau nÃ y
}

# ================================
# 2) LOAD EMBEDDER
# ================================
print("ğŸ§  Loading embedding model (Vietnamese-SBERT)...")
embedder = SentenceTransformer("keepitreal/vietnamese-sbert")

# ================================
# 3) NORMALIZER
# ================================
def normalize_text(text: str) -> str:
    text = text.strip()
    text = " ".join(text.split())
    return text

# ================================
# 4) LOAD DATA THEO ÄOáº N (Ráº¤T QUAN TRá»ŒNG)
# ================================
def load_paragraphs(path: str):
    """
    Má»—i Ä‘oáº¡n (paragraph) = 1 tÃ¬nh huá»‘ng = 1 embedding
    Cháº¥p nháº­n:
    - 1 Ä‘oáº¡n = 1 dÃ²ng
    - 1 Ä‘oáº¡n = nhiá»u dÃ²ng
    Miá»…n lÃ  cÃ¡ch nhau báº±ng dÃ²ng trá»‘ng
    """
    with open(path, "r", encoding="utf-8") as f:
        raw_text = f.read()

    paragraphs = [
        normalize_text(p)
        for p in re.split(r"\n\s*\n", raw_text)
        if p.strip()
    ]

    return paragraphs

# ================================
# 5) BUILD FAISS FOR EACH INTENT
# ================================
def build_for_intent(intent: str, filename: str):
    print(f"\n============================")
    print(f"ğŸ” Building FAISS for intent: {intent}")
    print(f"ğŸ“„ File: {filename}")

    path = os.path.join(DATA_DIR, filename)

    if not os.path.exists(path):
        print("âš  File khÃ´ng tá»“n táº¡i â†’ bá» qua")
        return

    docs = load_paragraphs(path)
    n_docs = len(docs)

    print(f"ğŸ“Œ Sá»‘ Ä‘oáº¡n load Ä‘Æ°á»£c: {n_docs}")

    # Cáº£nh bÃ¡o format
    if n_docs < 50:
        print("âš ï¸ Cáº¢NH BÃO: sá»‘ Ä‘oáº¡n quÃ¡ Ã­t â†’ cÃ³ thá»ƒ file bá»‹ dÃ­nh Ä‘oáº¡n!")

    # ================================
    # ENCODE
    # ================================
    embeddings = embedder.encode(
        docs,
        batch_size=64,
        convert_to_numpy=True,
        show_progress_bar=True,
        normalize_embeddings=True  # Báº®T BUá»˜C cho cosine
    )

    dim = embeddings.shape[1]

    # DÃ¹ng Inner Product vÃ¬ vector Ä‘Ã£ normalize
    index = faiss.IndexFlatIP(dim)
    index.add(embeddings)

    # ================================
    # SAVE
    # ================================
    index_path = os.path.join(EMB_DIR, f"{intent}_index.faiss")
    docs_path = os.path.join(EMB_DIR, f"{intent}_docs.pkl")

    faiss.write_index(index, index_path)

    with open(docs_path, "wb") as f:
        pickle.dump(docs, f)

    print(f"âœ… Saved FAISS index â†’ {index_path}")
    print(f"âœ… Saved docs        â†’ {docs_path}")

# ================================
# 6) RUN ALL INTENTS
# ================================
for intent, filename in INTENT_FILES.items():
    build_for_intent(intent, filename)

print("\nğŸ‰ DONE! Built FAISS for ALL INTENTS.")
